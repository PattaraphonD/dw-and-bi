from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyTableOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator

from airflow.utils import timezone
from datetime import datetime

import os
import json
import csv
import requests

from google.cloud import bigquery
from google.oauth2 import service_account

import logging
from airflow.models import Connection
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook


def _get_files_api():
    api_key = "DQdXFnbDsgMU3QEmQoDM9E1KvECWJNZK"
    headers = {"api-key": api_key}
    params = {"resource_id": "d1ab8a36-c5f7-4efb-b613-63310054b0bc"}

    response = requests.get("https://opend.data.go.th/get-ckan/datastore_search", params, headers=headers)
    data = response.json()
    records = data.get("result", {}).get("records", [])
    logging.info(records)

    # Extract year from current timestamp
    current_year = datetime.now().year

    file_name = f"RoadAccident_{current_year}.csv"
    file_path = os.path.join("/opt/airflow/dags", file_name)

    with open(file_path, mode="w", newline="", encoding="utf-8") as f:
        # Extract field names from the first record
        fieldnames = records[0].keys() if records else []
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        # Write the header
        writer.writeheader()
        # Write the records
        writer.writerows(records)

    return file_path

dag = DAG(
    "accident_api_pipeline",
    start_date=timezone.datetime(2024, 5, 4),
    schedule="@yearly",
    tags=["DS525"],
)

get_files_api = PythonOperator(
    task_id="get_files_api",
    python_callable=_get_files_api,
    dag=dag,
)

upload_to_gcs = LocalFilesystemToGCSOperator(
    task_id="upload_to_gcs",
    src=_get_files_api(),  # Using the file path generated by _get_files_api function
    dst="RoadAccident_{{ execution_date.year }}.csv",
    bucket="swu-ds525-8888",
    gcp_conn_id="my_gcp_conn",
    dag=dag,
)

create_bq_dataset = BigQueryCreateEmptyDatasetOperator(
    task_id='create_bq_dataset',
    dataset_id='project_accident',  # specify the dataset ID
    project_id='stalwart-summer-413911',  # specify your BigQuery project ID
    location='asia-southeast1',  # specify the location for the dataset
    gcp_conn_id='my_gcp_conn',  # specify the connection ID for GCP
    dag=dag,
)

# create_bq_table = BigQueryCreateEmptyTableOperator(
#     task_id='create_table',
#     dataset_id='proj_accident',  # specify the dataset where you want to create the table
#     table_id='accident_case',      # specify the table name
#     project_id='stalwart-summer-413911',  # specify your BigQuery project ID
#     gcp_conn_id='my_gcp_conn',  # specify the connection ID for GCP
#     dag=dag,
# )

load_to_bq_personal_info = GCSToBigQueryOperator(
    task_id='load_to_bq_personal_info',
    bucket='swu-ds525-8888',
    source_objects=['RoadAccident_{{ execution_date.year }}*.csv'],  # Use wildcard pattern
    destination_project_dataset_table='stalwart-summer-413911.project_accident.personal_info',
    source_format='CSV',
    schema_fields=[
        {'name': '_id', 'type': 'STRING'},
        {'name': 'dead_id', 'type': 'STRING'},
        {'name': 'dead_year', 'type': 'STRING'},
        {'name': 'dead_bdyear', 'type': 'STRING'},
        {'name': 'age', 'type': 'STRING'},
        {'name': 'sex', 'type': 'STRING'},
        # {'name': 'birth_year', 'type': 'STRING'},
        # {'name': 'nationality_id', 'type': 'STRING'},
        # {'name': 'tambon', 'type': 'STRING'},
        # {'name': 'district', 'type': 'STRING'},
        # {'name': 'province_name', 'type': 'STRING'},
    ],
    skip_leading_rows=1,  # If CSV has headers
    write_disposition='WRITE_TRUNCATE',  # Options: WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY
    gcp_conn_id="my_gcp_conn",
    dag=dag,
)

get_files_api >> upload_to_gcs >> create_bq_dataset >> load_to_bq_personal_info 